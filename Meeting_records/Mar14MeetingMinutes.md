**In Attendance:** Monica, Matthew, Ryan, Alina, Fritz, Ioan, Lydia
**Absent:** Caesar, Megan, Ashim

## 1st Agenda: Convert table document to .csv format
- The excel document has been removed (R.I.P.)
- From now on make changes in the .csv file (which you can edit in github, yay!)

## 2nd Agenda: Discuss opinion on columns
### Iona
- Is reviewing papers that are focused on methods development so data availability is not the most applicable
- Sample population is not very applicable
- Description of methods and tool accessibility is very applicable

### Fritz
-	Sample population could be merged into one of the columns, maybe quality of experimental design
-	Data and tool accessibility are really important

### Lydia
- Data and tool accessibility were very important and easy to evaluate
- Had troubles separating sample population from experimental desing

### Alina
-	Data and description of method easy to fill
-	Sample population – a bit confusing
-	Tool accessibility – wasn’t sure how to assess 

### Ryan
-	Data availability and description of methods worked well for one paper
-	For other paper quality of experimental design was more important
-	Maybe should use not applicable for some columns

### Monica
-	Data availability, tool accessibility were great
-	For tool accessibility – we should not go too much into cost?
-	Three middle columns could maybe become 2
-	Replicability – are we worried about it for this particular project?

### Matthew
-	Tool accessibility very important
-	Data availability fairly important
-	Quality of experimental design, methods, and sample population somewhat overlap
-	Maybe combine methods and sample population
-	For public health, replicability is very important

## 3rd Agenda: Reproducibility vs Replicability
-	Reproducibility = original data and code is used to regenerate the results
-	Replicability = collect new data and arrive at the same finding 
-	This difference should be discussed in the intro of our report
-	For our purposes we can critically judge reproducibility because it is really based on the availability of data and code, and whether analysis methods are described in enough detail
-	Replicability we can make an inference about but can’t test ourselves for the purpose of this assignment

**Final decision:**

Undecided?

What are peoples thoughts?

Lydia: I think it is important to discuss replicability (for some fields perhaps more than others). I think it is difficult though because we are not reviewing the paper's technique. Maybe in a section of the report we can talk about various signs to look for in a paper to assess replicability and include a section in the markdown on replicability for each paper. In the table it can maybe be a column we give a categorical score (ex: Likely to replicate, potentially difficult to replicate, Likely will not replicate)?

## 4th Agenda: Column Changes 
- Keeping Data availability, tool accessibility, and quality of design as is.
- Going to merge description of sample populations/data of study with Description of methods

## 5th Agenda: Criteria for Columns
### Data availability
First we tried to make a decision tree but it got too complicated (I think someone has a picture of it)

Decided to score based on how long it could take to get access to the data (faster = better) because this definition can be generalized to all fields.

3 = Immediate access (links to repositories, access is free, etc.)

2 = Eventual access with uncertain timeline (data is confidential and must go through approving agency, paywall to data, statement saying you can reach out for data access but you don't know how quickly they will respond etc.)

1 = Uncertain access with uncertain timeline (Will need to contact someone and you don't know how they will respond or if they will respond, etc.)

**For the other columns we did not get to making a specific rubric like we have for data availability. We ended with thinking about lumping description of protocols, samples, and tools under description of methods but this idea hasn't been fully figured out. We are going to think on our own about how we can make generalizable criteria for the other categories and reconvene on Tuesday March 19 @ 1:30.**

## Additional Notes
### Assessing quality of experimental design
 - Does this experiment answer the research question?
 - Are the authors making assumptions that are not based on the evidence they acquried?
 - Is the hypothesis answered in a scientific way?

### Description of methods
- Is the path of answering the hypothesis explained completely?

## If you have additional thoughts to share, write them out below

